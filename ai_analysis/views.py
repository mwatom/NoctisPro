from django.shortcuts import render, get_object_or_404, redirect
from django.contrib.auth.decorators import login_required, user_passes_test
from django.http import JsonResponse, HttpResponse
from django.views.decorators.csrf import csrf_exempt
from django.contrib import messages
from django.db.models import Q, Count, Avg
from django.core.paginator import Paginator
from django.utils import timezone
from datetime import datetime, timedelta
import json
import numpy as np
import pydicom
import os
import threading
import time
import re
import requests
try:
    import onnxruntime as ort
except Exception:
    ort = None
try:
    from transformers import AutoTokenizer, AutoModelForSequenceClassification
except Exception:
    AutoTokenizer = None
    AutoModelForSequenceClassification = None

from worklist.models import Study, DicomImage, Series
from accounts.models import User
from .models import (
    AIModel, AIAnalysis, AutoReportTemplate, AutoGeneratedReport,
    AITrainingData, AIPerformanceMetric, AIFeedback
)

def is_admin_or_radiologist(user):
    """Check if user is admin or radiologist"""
    return user.is_authenticated and (user.is_admin() or user.is_radiologist())

@login_required
def ai_dashboard(request):
    """AI analysis dashboard with comprehensive overview"""
    user = request.user
    
    # Get AI models statistics
    total_models = AIModel.objects.filter(is_active=True).count()
    active_analyses = AIAnalysis.objects.filter(status__in=['pending', 'processing']).count()
    completed_today = AIAnalysis.objects.filter(
        completed_at__date=timezone.now().date(),
        status='completed'
    ).count()
    
    # Get recent analyses
    if user.is_facility_user():
        recent_analyses = AIAnalysis.objects.filter(
            study__facility=user.facility
        ).select_related('study', 'ai_model').order_by('-requested_at')[:10]
    else:
        recent_analyses = AIAnalysis.objects.select_related(
            'study', 'ai_model'
        ).order_by('-requested_at')[:10]
    
    # Get pending auto-reports
    if user.is_radiologist() or user.is_admin():
        pending_reports = AutoGeneratedReport.objects.filter(
            review_status='pending'
        ).select_related('study', 'ai_analysis').order_by('-generated_at')[:5]
    else:
        pending_reports = []
    
    # Get model performance summary
    model_performance = []
    for model in AIModel.objects.filter(is_active=True)[:5]:
        latest_metric = model.performance_metrics.first()
        model_performance.append({
            'model': model,
            'accuracy': latest_metric.accuracy if latest_metric else 0,
            'total_analyses': model.total_analyses,
            'avg_time': model.avg_processing_time
        })
    
    context = {
        'total_models': total_models,
        'active_analyses': active_analyses,
        'completed_today': completed_today,
        'recent_analyses': recent_analyses,
        'pending_reports': pending_reports,
        'model_performance': model_performance,
        'user': user,
    }
    
    return render(request, 'ai_analysis/dashboard.html', context)

@login_required
@csrf_exempt
def analyze_study(request, study_id):
    """Run AI analysis on study"""
    study = get_object_or_404(Study, id=study_id)
    user = request.user
    
    # Check permissions
    if user.is_facility_user() and study.facility != user.facility:
        return JsonResponse({'error': 'Permission denied'}, status=403)
    
    if request.method == 'POST':
        # Only administrators or radiologists can initiate new AI analyses
        if not is_admin_or_radiologist(user):
            return JsonResponse({'error': 'Only administrators or radiologists can start AI analyses'}, status=403)
        try:
            # Get selected AI models
            model_ids = request.POST.getlist('ai_models')
            priority = request.POST.get('priority', 'normal')
            
            if not model_ids:
                return JsonResponse({'error': 'Please select at least one AI model'}, status=400)
            
            # Create analyses for each selected model
            analyses = []
            for model_id in model_ids:
                ai_model = get_object_or_404(AIModel, id=model_id, is_active=True)
                
                # Check if analysis already exists
                existing = AIAnalysis.objects.filter(
                    study=study,
                    ai_model=ai_model,
                    status__in=['pending', 'processing', 'completed']
                ).first()
                
                if existing:
                    continue
                
                # Create new analysis
                analysis = AIAnalysis.objects.create(
                    study=study,
                    ai_model=ai_model,
                    priority=priority,
                    status='pending'
                )
                analyses.append(analysis)
            
            # Start processing in background
            if analyses:
                threading.Thread(
                    target=process_ai_analyses,
                    args=(analyses,),
                    daemon=True
                ).start()
            
            return JsonResponse({
                'success': True,
                'message': f'Started AI analysis with {len(analyses)} models',
                'analysis_ids': [a.id for a in analyses]
            })
            
        except Exception as e:
            return JsonResponse({'error': str(e)}, status=500)
    
    # GET request - show analysis form
    available_models = []
    if is_admin_or_radiologist(user):
        available_models = AIModel.objects.filter(
            is_active=True,
            modality__in=[study.modality.code, 'ALL']
        )
    
    # Get existing analyses
    existing_analyses = AIAnalysis.objects.filter(
        study=study
    ).select_related('ai_model').order_by('-requested_at')
    
    context = {
        'study': study,
        'available_models': available_models,
        'existing_analyses': existing_analyses,
    }
    
    return render(request, 'ai_analysis/analyze_study.html', context)

@login_required
@csrf_exempt
def api_analysis_status(request, analysis_id):
    """Get analysis status and progress"""
    analysis = get_object_or_404(AIAnalysis, id=analysis_id)
    user = request.user
    
    # Check permissions
    if user.is_facility_user() and analysis.study.facility != user.facility:
        return JsonResponse({'error': 'Permission denied'}, status=403)
    
    progress_percentage = 0
    if analysis.status == 'completed':
        progress_percentage = 100
    elif analysis.status == 'processing':
        # Estimate progress based on processing time
        if analysis.started_at:
            elapsed = (timezone.now() - analysis.started_at).total_seconds()
            estimated_total = analysis.ai_model.avg_processing_time or 60
            progress_percentage = min(90, (elapsed / estimated_total) * 100)
    
    # Only administrators/radiologists get full detailed analysis
    clinician = is_admin_or_radiologist(user)
    data = {
        'id': analysis.id,
        'status': analysis.status,
        'progress_percentage': round(progress_percentage, 2),
        'confidence_score': analysis.confidence_score,
        'requested_at': analysis.requested_at.isoformat(),
        'completed_at': analysis.completed_at.isoformat() if analysis.completed_at else None,
        'ai_model': {
            'name': analysis.ai_model.name,
            'version': analysis.ai_model.version,
            'type': analysis.ai_model.model_type
        }
    }
    if clinician:
        data.update({
            'findings': analysis.findings,
            'abnormalities_detected': analysis.abnormalities_detected,
            'measurements': analysis.measurements,
            'processing_time': analysis.processing_time,
            'error_message': analysis.error_message,
        })
    else:
        # Minimal, non-intrusive preliminary summary for non-clinician roles
        data.update({
            'summary': 'Preliminary AI review complete' if analysis.status == 'completed' else 'AI review in progress',
        })
    
    return JsonResponse(data)

@login_required
@user_passes_test(is_admin_or_radiologist)
@csrf_exempt
def generate_auto_report(request, study_id):
    """Generate automatic report from AI analysis"""
    study = get_object_or_404(Study, id=study_id)
    user = request.user
    
    # Check facility permissions
    if user.is_facility_user() and study.facility != user.facility:
        return JsonResponse({'error': 'Permission denied'}, status=403)
    
    try:
        # Get completed AI analyses for this study
        analyses = AIAnalysis.objects.filter(
            study=study,
            status='completed'
        ).select_related('ai_model')
        
        if not analyses:
            return JsonResponse({'error': 'No completed AI analyses found for this study'}, status=400)
        
        # Find appropriate template
        template = AutoReportTemplate.objects.filter(
            modality=study.modality.code,
            body_part__icontains=study.body_part,
            is_active=True
        ).first()
        
        if not template:
            # Use generic template
            template = AutoReportTemplate.objects.filter(
                modality=study.modality.code,
                is_active=True
            ).first()
        
        if not template:
            return JsonResponse({'error': 'No suitable report template found'}, status=400)
        
        # Generate report content
        report_data = generate_report_content(study, analyses, template)
        
        # Create auto-generated report
        auto_report = AutoGeneratedReport.objects.create(
            study=study,
            template=template,
            ai_analysis=analyses.first(),  # Primary analysis
            generated_findings=report_data['findings'],
            generated_impression=report_data['impression'],
            generated_recommendations=report_data['recommendations'],
            overall_confidence=report_data['confidence'],
            requires_review=report_data['confidence'] < template.confidence_threshold
        )
        
        return JsonResponse({
            'success': True,
            'report_id': auto_report.id,
            'findings': auto_report.generated_findings,
            'impression': auto_report.generated_impression,
            'recommendations': auto_report.generated_recommendations,
            'confidence': auto_report.overall_confidence,
            'requires_review': auto_report.requires_review
        })
        
    except Exception as e:
        return JsonResponse({'error': f'Error generating report: {str(e)}'}, status=500)

@login_required
@user_passes_test(is_admin_or_radiologist)
def review_auto_report(request, report_id):
    """Review and approve/modify auto-generated report"""
    auto_report = get_object_or_404(AutoGeneratedReport, id=report_id)
    user = request.user
    
    # Check permissions
    if user.is_facility_user() and auto_report.study.facility != user.facility:
        messages.error(request, 'Permission denied')
        return redirect('ai_analysis:ai_dashboard')
    
    if request.method == 'POST':
        action = request.POST.get('action')
        
        if action == 'approve':
            auto_report.review_status = 'approved'
            auto_report.reviewed_by = user
            auto_report.reviewed_at = timezone.now()
            auto_report.review_comments = request.POST.get('comments', '')
            auto_report.save()
            
            # Create final report (when reports app is available)
            # auto_report.approve_and_create_report(user)
            
            messages.success(request, 'Auto-generated report approved successfully')
            
        elif action == 'modify':
            auto_report.generated_findings = request.POST.get('findings')
            auto_report.generated_impression = request.POST.get('impression')
            auto_report.generated_recommendations = request.POST.get('recommendations')
            auto_report.review_status = 'modified'
            auto_report.reviewed_by = user
            auto_report.reviewed_at = timezone.now()
            auto_report.review_comments = request.POST.get('comments', '')
            auto_report.save()
            
            messages.success(request, 'Auto-generated report modified and approved')
            
        elif action == 'reject':
            auto_report.review_status = 'rejected'
            auto_report.reviewed_by = user
            auto_report.reviewed_at = timezone.now()
            auto_report.review_comments = request.POST.get('comments', '')
            auto_report.save()
            
            messages.success(request, 'Auto-generated report rejected')
        
        return redirect('ai_analysis:ai_dashboard')
    
    # GET request - show review form
    context = {
        'auto_report': auto_report,
        'study': auto_report.study,
        'ai_analysis': auto_report.ai_analysis,
    }
    
    return render(request, 'ai_analysis/review_auto_report.html', context)

@login_required
@csrf_exempt
def api_ai_feedback(request, analysis_id):
    """Submit feedback on AI analysis"""
    analysis = get_object_or_404(AIAnalysis, id=analysis_id)
    user = request.user
    
    # Check permissions
    if user.is_facility_user() and analysis.study.facility != user.facility:
        return JsonResponse({'error': 'Permission denied'}, status=403)
    
    if request.method == 'POST':
        try:
            data = json.loads(request.body)
            
            feedback = AIFeedback.objects.create(
                ai_analysis=analysis,
                user=user,
                feedback_type=data.get('feedback_type'),
                rating=data.get('rating'),
                comments=data.get('comments', ''),
                incorrect_findings=data.get('incorrect_findings', []),
                missed_findings=data.get('missed_findings', []),
                suggestions=data.get('suggestions', '')
            )
            
            return JsonResponse({
                'success': True,
                'feedback_id': feedback.id,
                'message': 'Feedback submitted successfully'
            })
            
        except Exception as e:
            return JsonResponse({'error': str(e)}, status=500)
    
    return JsonResponse({'error': 'Method not allowed'}, status=405)

@login_required
@user_passes_test(lambda u: u.is_admin())
def model_management(request):
    """AI model management interface"""
    models = AIModel.objects.all().order_by('-created_at')
    
    # Search and filtering
    search_query = request.GET.get('search', '')
    if search_query:
        models = models.filter(
            Q(name__icontains=search_query) |
            Q(description__icontains=search_query) |
            Q(modality__icontains=search_query)
        )
    
    model_type_filter = request.GET.get('model_type', '')
    if model_type_filter:
        models = models.filter(model_type=model_type_filter)
    
    # Pagination
    paginator = Paginator(models, 20)
    page_number = request.GET.get('page')
    models_page = paginator.get_page(page_number)
    
    context = {
        'models': models_page,
        'search_query': search_query,
        'model_type_filter': model_type_filter,
        'model_types': AIModel.MODEL_TYPES,
    }
    
    return render(request, 'ai_analysis/model_management.html', context)

@login_required
@csrf_exempt
def api_realtime_analyses(request):
    """Get real-time AI analysis updates"""
    user = request.user
    
    # Get timestamp from request
    last_update = request.GET.get('last_update')
    
    try:
        if last_update:
            last_update_time = timezone.datetime.fromisoformat(last_update.replace('Z', '+00:00'))
        else:
            last_update_time = timezone.now() - timezone.timedelta(minutes=5)
    except:
        last_update_time = timezone.now() - timezone.timedelta(minutes=5)
    
    # Get analyses updated since last check
    if user.is_facility_user():
        analyses = AIAnalysis.objects.filter(
            study__facility=user.facility,
            requested_at__gt=last_update_time
        ).select_related('study', 'ai_model').order_by('-requested_at')[:20]
    else:
        analyses = AIAnalysis.objects.filter(
            requested_at__gt=last_update_time
        ).select_related('study', 'ai_model').order_by('-requested_at')[:20]
    
    clinician = is_admin_or_radiologist(user)
    analyses_data = []
    for analysis in analyses:
        item = {
            'id': analysis.id,
            'study_id': analysis.study.id,
            'accession_number': analysis.study.accession_number,
            'patient_name': analysis.study.patient.full_name,
            'ai_model': analysis.ai_model.name,
            'status': analysis.status,
            'priority': analysis.priority,
            'requested_at': analysis.requested_at.isoformat(),
            'completed_at': analysis.completed_at.isoformat() if analysis.completed_at else None,
        }
        if clinician:
            item['confidence_score'] = analysis.confidence_score
        analyses_data.append(item)
    
    return JsonResponse({
        'analyses': analyses_data,
        'timestamp': timezone.now().isoformat(),
        'count': len(analyses_data)
    })

@login_required
@user_passes_test(lambda u: u.is_admin())
def ai_reporting_dashboard(request):
    """Comprehensive AI reporting dashboard"""
    # Get date range from request
    end_date = timezone.now().date()
    start_date = end_date - timedelta(days=30)
    
    date_filter = request.GET.get('date_range', '30d')
    if date_filter == '7d':
        start_date = end_date - timedelta(days=7)
    elif date_filter == '90d':
        start_date = end_date - timedelta(days=90)
    elif date_filter == '1y':
        start_date = end_date - timedelta(days=365)
    
    # Overall statistics
    total_analyses = AIAnalysis.objects.filter(
        requested_at__date__gte=start_date,
        requested_at__date__lte=end_date
    ).count()
    
    completed_analyses = AIAnalysis.objects.filter(
        requested_at__date__gte=start_date,
        requested_at__date__lte=end_date,
        status='completed'
    ).count()
    
    failed_analyses = AIAnalysis.objects.filter(
        requested_at__date__gte=start_date,
        requested_at__date__lte=end_date,
        status='failed'
    ).count()
    
    # Success rate
    success_rate = (completed_analyses / total_analyses * 100) if total_analyses > 0 else 0
    
    # Average processing time
    avg_processing_time = AIAnalysis.objects.filter(
        requested_at__date__gte=start_date,
        requested_at__date__lte=end_date,
        status='completed',
        processing_time__isnull=False
    ).aggregate(Avg('processing_time'))['processing_time__avg'] or 0
    
    # Model performance breakdown
    model_performance = []
    for model in AIModel.objects.filter(is_active=True):
        model_analyses = AIAnalysis.objects.filter(
            ai_model=model,
            requested_at__date__gte=start_date,
            requested_at__date__lte=end_date
        )
        
        total = model_analyses.count()
        completed = model_analyses.filter(status='completed').count()
        failed = model_analyses.filter(status='failed').count()
        avg_confidence = model_analyses.filter(
            status='completed',
            confidence_score__isnull=False
        ).aggregate(Avg('confidence_score'))['confidence_score__avg'] or 0
        
        model_performance.append({
            'model': model,
            'total_analyses': total,
            'completed': completed,
            'failed': failed,
            'success_rate': (completed / total * 100) if total > 0 else 0,
            'avg_confidence': round(avg_confidence, 3),
            'avg_processing_time': model.avg_processing_time or 0
        })
    
    # Auto-report statistics
    total_auto_reports = AutoGeneratedReport.objects.filter(
        generated_at__date__gte=start_date,
        generated_at__date__lte=end_date
    ).count()
    
    approved_reports = AutoGeneratedReport.objects.filter(
        generated_at__date__gte=start_date,
        generated_at__date__lte=end_date,
        review_status='approved'
    ).count()
    
    pending_reports = AutoGeneratedReport.objects.filter(
        review_status='pending'
    ).count()
    
    # Daily analysis trends
    daily_trends = []
    current_date = start_date
    while current_date <= end_date:
        daily_count = AIAnalysis.objects.filter(
            requested_at__date=current_date
        ).count()
        daily_trends.append({
            'date': current_date.isoformat(),
            'count': daily_count
        })
        current_date += timedelta(days=1)
    
    # Top performing models
    top_models = sorted(model_performance, key=lambda x: x['success_rate'], reverse=True)[:5]
    
    # Recent feedback
    recent_feedback = AIFeedback.objects.filter(
        created_at__date__gte=start_date
    ).select_related('ai_analysis__ai_model', 'user').order_by('-created_at')[:10]
    
    context = {
        'total_analyses': total_analyses,
        'completed_analyses': completed_analyses,
        'failed_analyses': failed_analyses,
        'success_rate': round(success_rate, 2),
        'avg_processing_time': round(avg_processing_time, 2),
        'model_performance': model_performance,
        'total_auto_reports': total_auto_reports,
        'approved_reports': approved_reports,
        'pending_reports': pending_reports,
        'daily_trends': daily_trends,
        'top_models': top_models,
        'recent_feedback': recent_feedback,
        'date_filter': date_filter,
        'start_date': start_date,
        'end_date': end_date,
    }
    
    return render(request, 'ai_analysis/reporting_dashboard.html', context)

@login_required
@user_passes_test(lambda u: u.is_admin())
def verify_ai_models(request):
    """Verify that all AI models are working correctly"""
    verification_results = []
    
    # Get all active AI models
    models = AIModel.objects.filter(is_active=True)
    
    for model in models:
        result = {
            'model': model,
            'status': 'unknown',
            'last_test': None,
            'test_results': {},
            'issues': []
        }
        
        # Check if model files exist
        if not os.path.exists(model.model_file_path):
            result['issues'].append('Model file not found')
            result['status'] = 'error'
        
        # Check recent analyses
        recent_analyses = AIAnalysis.objects.filter(
            ai_model=model,
            requested_at__gte=timezone.now() - timedelta(days=7)
        )
        
        if recent_analyses.exists():
            completed = recent_analyses.filter(status='completed').count()
            failed = recent_analyses.filter(status='failed').count()
            total = recent_analyses.count()
            
            success_rate = (completed / total * 100) if total > 0 else 0
            
            result['test_results'] = {
                'total_tests': total,
                'completed': completed,
                'failed': failed,
                'success_rate': round(success_rate, 2)
            }
            
            if success_rate >= 95:
                result['status'] = 'excellent'
            elif success_rate >= 85:
                result['status'] = 'good'
            elif success_rate >= 70:
                result['status'] = 'warning'
            else:
                result['status'] = 'error'
                result['issues'].append(f'Low success rate: {success_rate:.1f}%')
        else:
            result['status'] = 'untested'
            result['issues'].append('No recent test data available')
        
        # Check performance metrics
        latest_metric = model.performance_metrics.first()
        if latest_metric:
            result['last_test'] = latest_metric.evaluation_date
            if latest_metric.accuracy < 0.8:
                result['issues'].append(f'Low accuracy: {latest_metric.accuracy:.3f}')
        else:
            result['issues'].append('No performance metrics available')
        
        verification_results.append(result)
    
    # Overall system status
    statuses = [r['status'] for r in verification_results]
    if 'error' in statuses:
        overall_status = 'error'
    elif 'warning' in statuses:
        overall_status = 'warning'
    elif 'untested' in statuses:
        overall_status = 'warning'
    else:
        overall_status = 'good'
    
    context = {
        'verification_results': verification_results,
        'overall_status': overall_status,
        'total_models': len(verification_results),
        'error_count': statuses.count('error'),
        'warning_count': statuses.count('warning') + statuses.count('untested'),
        'good_count': statuses.count('good') + statuses.count('excellent'),
    }
    
    return render(request, 'ai_analysis/model_verification.html', context)

@login_required
@user_passes_test(lambda u: u.is_admin())
@csrf_exempt
def run_model_test(request, model_id):
    """Run a test on a specific AI model"""
    model = get_object_or_404(AIModel, id=model_id)
    
    if request.method == 'POST':
        try:
            # Get a test study for this modality
            test_study = Study.objects.filter(
                modality__code=model.modality
            ).first()
            
            if not test_study:
                return JsonResponse({
                    'success': False,
                    'error': f'No test studies available for modality {model.modality}'
                })
            
            # Create a test analysis
            test_analysis = AIAnalysis.objects.create(
                study=test_study,
                ai_model=model,
                priority='high',
                status='pending'
            )
            
            # Run the analysis in background
            threading.Thread(
                target=process_ai_analyses,
                args=([test_analysis],),
                daemon=True
            ).start()
            
            return JsonResponse({
                'success': True,
                'analysis_id': test_analysis.id,
                'message': f'Test started for {model.name}'
            })
            
        except Exception as e:
            return JsonResponse({
                'success': False,
                'error': str(e)
            })
    
    return JsonResponse({'error': 'Method not allowed'}, status=405)

def process_ai_analyses(analyses):
    """Background task to process AI analyses"""
    for analysis in analyses:
        try:
            analysis.start_processing()
            
            # Simulate AI processing (replace with actual AI model inference)
            results = simulate_ai_analysis(analysis)
            
            # Complete the analysis
            analysis.complete_analysis(results)
            
            # Update model statistics
            model = analysis.ai_model
            model.total_analyses += 1
            if analysis.processing_time:
                # Update average processing time
                if model.avg_processing_time > 0:
                    model.avg_processing_time = (
                        model.avg_processing_time + analysis.processing_time
                    ) / 2
                else:
                    model.avg_processing_time = analysis.processing_time
            model.save()
            
        except Exception as e:
            analysis.status = 'failed'
            analysis.error_message = str(e)
            analysis.save()

def simulate_ai_analysis(analysis):
    """Heavier inference if available; otherwise safe simulation."""
    modality = analysis.study.modality.code
    # Heavier text classification demo for AI summary confidence, if transformers available
    confidence = 0.85
    if AutoTokenizer and AutoModelForSequenceClassification:
        try:
            # Lightweight sentiment-like proxy to modulate confidence
            model_name = 'distilbert-base-uncased-finetuned-sst-2-english'
            tokenizer = AutoTokenizer.from_pretrained(model_name)
            model = AutoModelForSequenceClassification.from_pretrained(model_name)
            text = f"Preliminary {modality} analysis"
            inputs = tokenizer(text, return_tensors='pt')
            outputs = model(**inputs)
            scores = outputs.logits.softmax(dim=-1).detach().numpy()[0]
            confidence = float(scores.max()) * 0.2 + 0.8  # keep range ~0.8-1.0
        except Exception:
            confidence = 0.88
    # Optional ONNX path could go here for imaging if an .onnx exists; skip unless file provided
    time.sleep(2)
    if modality == 'CT':
        findings = "No acute intracranial abnormality. Brain parenchyma appears normal."
        abnormalities = []
        measurements = {"brain_volume": "1450 mL", "ventricle_size": "normal"}
    elif modality == 'MR':
        findings = "Normal brain MRI. No evidence of acute infarction or hemorrhage."
        abnormalities = []
        measurements = {"lesion_count": 0, "white_matter": "normal"}
    elif modality == 'XR':
        findings = "Chest X-ray shows clear lungs. Heart size is normal."
        abnormalities = []
        measurements = {"heart_size": "normal", "lung_fields": "clear"}
    else:
        findings = "Study reviewed by AI. No acute abnormalities detected."
        abnormalities = []
        measurements = {}
    return {
        'findings': findings,
        'abnormalities': abnormalities,
        'confidence': confidence,
        'measurements': measurements
    }

def generate_report_content(study, analyses, template):
    """Generate report content from AI analyses"""
    # Aggregate findings from all analyses
    all_findings = []
    all_abnormalities = []
    confidence_scores = []
    
    for analysis in analyses:
        if analysis.findings:
            all_findings.append(f"[{analysis.ai_model.name}] {analysis.findings}")
        if analysis.abnormalities_detected:
            all_abnormalities.extend(analysis.abnormalities_detected)
        if analysis.confidence_score:
            confidence_scores.append(analysis.confidence_score)
    
    # Calculate overall confidence
    overall_confidence = np.mean(confidence_scores) if confidence_scores else 0.5
    
    # Generate findings section
    findings_text = template.findings_template.format(
        patient_name=study.patient.full_name,
        study_date=study.study_date.strftime('%Y-%m-%d'),
        modality=study.modality.name,
        findings='; '.join(all_findings) if all_findings else 'No significant findings detected.'
    )
    
    # Generate impression
    if all_abnormalities:
        impression_text = f"Abnormalities detected: {', '.join([str(a) for a in all_abnormalities])}"
    else:
        impression_text = "No acute abnormalities detected by AI analysis."
    
    # Generate recommendations
    recommendations_text = template.recommendations_template or "Recommend correlation with clinical findings."
    
    # Add a professional nudge to encourage clinician review and research
    research_nudge = ("Note: This AI-generated summary is for preliminary support only. "
                      "Please review images directly, correlate clinically, and consult authoritative references. "
                      "Consider reviewing current guidelines and differential diagnoses relevant to the findings.")
    
    return {
        'findings': findings_text + "\n\n" + research_nudge,
        'impression': impression_text,
        'recommendations': recommendations_text,
        'confidence': overall_confidence
    }


@login_required
@csrf_exempt
def api_medical_references(request):
    """
    Provide curated external references based on query keywords (e.g., suspected finding/body part/modality).
    Returns titles and URLs from public reputable sources where possible.
    """
    try:
        query = request.GET.get('q', '').strip()
        if not query:
            return JsonResponse({'success': False, 'error': 'Missing query parameter q'}, status=400)

        # Basic source filters (avoid scraping TOS-sensitive sites; use public pages where permissible)
        sources = [
            'site:radiopaedia.org',
            'site:nih.gov',
            'site:ncbi.nlm.nih.gov',
            'site:who.int',
            'site:rsna.org'
        ]
        search_query = f"{query} ({' OR '.join(sources)})"

        # Use a simple web search via DuckDuckGo HTML (no API key); degrade gracefully if blocked
        ddg_url = 'https://duckduckgo.com/html/'
        params = { 'q': search_query }
        headers = { 'User-Agent': 'Mozilla/5.0 (compatible; NoctisPro/1.0)' }
        results = []
        try:
            resp = requests.post(ddg_url, data=params, headers=headers, timeout=8)
            if resp.ok:
                html = resp.text
                # Parse minimal anchors
                for m in re.finditer(r'<a[^>]+class="result__a"[^>]*href="([^"]+)"[^>]*>(.*?)</a>', html, re.I | re.S):
                    url = m.group(1)
                    title_raw = re.sub('<[^<]+?>', '', m.group(2))
                    title = re.sub('\s+', ' ', title_raw).strip()
                    if url and title:
                        results.append({'title': title, 'url': url})
                    if len(results) >= 8:
                        break
        except Exception:
            # If search unavailable, return empty list
            results = []

        return JsonResponse({
            'success': True,
            'query': query,
            'references': results
        })
    except Exception as e:
        return JsonResponse({'success': False, 'error': str(e)}, status=500)
